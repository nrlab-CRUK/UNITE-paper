{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# cv parameters\n",
    "num_folds = 5 \n",
    "num_repeats = 10 \n",
    "\n",
    "# model parameters\n",
    "optimizer = 'adam'\n",
    "learning_rate = 0.0008\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# training parameters\n",
    "epochs = 40\n",
    "batch_size = 32\n",
    "metrics=['accuracy',\n",
    "        tf.keras.metrics.AUC(curve = \"ROC\", name='AUROC'),\n",
    "        tf.keras.metrics.AUC(curve = \"PR\", name='AUPRC'),\n",
    "         tf.keras.metrics.Precision(name='precision'),\n",
    "         tf.keras.metrics.Recall(name='recall'),\n",
    "         tf.keras.metrics.SensitivityAtSpecificity(specificity=0.95, name=\"sen_at_0.95spec\"),\n",
    "         tf.keras.metrics.SensitivityAtSpecificity(specificity=0.98, name=\"sen_at_0.98spec\")]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_file = 'x_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.03_0.05_.npy'\n",
    "y_file = 'y_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.03_0.05_.npy' \n",
    "z_file = 'z_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.03_0.05_.npy'\n",
    "\n",
    "performance_csv = 'performance_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.03_0.05_.csv'\n",
    "\n",
    "x_file = 'x_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.01_0.03_.npy'\n",
    "y_file = 'y_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.01_0.03_.npy' \n",
    "z_file = 'z_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.01_0.03_.npy'\n",
    "\n",
    "performance_csv = 'performance_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.01_0.03_.csv'\n",
    "\n",
    "x = np.load(x_file, allow_pickle=True)\n",
    "y = np.load(y_file, allow_pickle=True)\n",
    "z = np.load(z_file, allow_pickle=True)\n",
    "\n",
    "y = np.array(y)\n",
    "input_shape = x.shape[1:]\n",
    "#input_shape = (465, 201, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import scikeras.wrappers as sw\n",
    "#from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "def create_model(input_shape, loss, learning_rate, metrics):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=input_shape),\n",
    "            \n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(3, 2)),\n",
    "            \n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            \n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            \n",
    "            layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            \n",
    "            layers.Flatten(),\n",
    "            layers.Dense(32, activation=\"relu\"),\n",
    "            #layers.Dropout(0.2),\n",
    "            layers.Dense(1, activation=\"sigmoid\")\n",
    "        ])\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss, \n",
    "        optimizer=opt, \n",
    "        metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "# compute class weights and set dictionary to class_weight parameter\n",
    "from sklearn.utils import class_weight\n",
    "class_weight = dict(zip(np.unique(y), class_weight.compute_class_weight(class_weight = 'balanced', \n",
    "                                                                        classes = np.unique(y), \n",
    "                                                                        y = y)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = sw.KerasClassifier(model = create_model,\t\n",
    "\t\t\tmodel__metrics = metrics,\n",
    "\t\t\tmodel__input_shape = input_shape,\n",
    "            model__loss = loss,\n",
    "            model__learning_rate = learning_rate,\n",
    "            class_weight=class_weight,\n",
    "            validation_split=0.2,\n",
    "            epochs=epochs, \n",
    "            batch_size=batch_size, \n",
    "            verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define scoring function by recall\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# define confusion_matrix_scorer, return specificity, sensitivity, f1, recall, precision and accuracy in a named dictionary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def scoring_1(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    f1 = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "    recall = sensitivity\n",
    "    precision = tp / (tp+fp)\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    return {'specificity': specificity, \n",
    "    \t\t'sensitivity': sensitivity, \n",
    "\t\t'f1': f1, \n",
    "\t\t'recall': recall, \n",
    "\t\t'precision': precision, \n",
    "\t\t'accuracy': accuracy}\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def sensitivity_at_specificity(y_true, y_pred_proba, specificity):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred_proba >= 0.5).ravel()\n",
    "    actual_negatives = tn + fp\n",
    "    actual_positives = fn + tp\n",
    "    tn_rate = tn / actual_negatives\n",
    "    fp_rate = fp / actual_negatives\n",
    "    cutoff = 0.5\n",
    "    while fp_rate > (1-specificity) and cutoff > 0:\n",
    "        cutoff -= 0.01\n",
    "        y_pred = (y_pred_proba > cutoff).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        actual_negatives = tn + fp\n",
    "        actual_positives = fn + tp\n",
    "        tn_rate = tn / actual_negatives\n",
    "        fp_rate = fp / actual_negatives\n",
    "    sensitivity = tp / actual_positives\n",
    "    return sensitivity\n",
    "\n",
    "\n",
    "\n",
    "scoring_2 = {'specificity': make_scorer(recall_score, pos_label=0),\n",
    "\t'sensitivity': make_scorer(recall_score, pos_label=1),\n",
    "\t'acc': 'accuracy',\n",
    "\t'f1': 'f1',\n",
    "\t'ppv': 'precision',\n",
    "\t'roc_auc': 'roc_auc',\n",
    "\t'pr_auc': 'average_precision',\n",
    "\t'sen_at_0.95spec': make_scorer(sensitivity_at_specificity,  specificity=0.95, needs_proba=True),\n",
    "\t'sen_at_0.98spec': make_scorer(sensitivity_at_specificity, specificity=0.98, needs_proba=True)}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use cross_validate to return multiple metrics\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold \n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "kfold = RepeatedStratifiedKFold(n_splits=num_folds,\n",
    "                                n_repeats=num_repeats,\n",
    "                                random_state=1)\n",
    "\n",
    "cv_results = cross_validate(model, \n",
    "                          x, y, \n",
    "                          scoring= scoring_2,\n",
    "                          return_train_score=True, \n",
    "                          cv=kfold,\n",
    "                          verbose=2,\n",
    "                          return_estimator=True,\n",
    "                          n_jobs=1)\n",
    "\n",
    "# use cross validate and stratifiedGroupKFold \n",
    "# repeat 10 times, 5 folds\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "kfold_grouped = StratifiedGroupKFold(n_splits=num_folds,\n",
    "                            shuffle=True)\n",
    "\n",
    "cv_results2 = cross_validate(model, \n",
    "                          x, y, \n",
    "                          groups = z,\n",
    "                          scoring= scoring_2,\n",
    "                          return_train_score=True, \n",
    "                          cv=kfold_grouped,\n",
    "                          verbose=2,\n",
    "                          return_estimator=True,\n",
    "                          n_jobs=1)\n",
    "\n",
    "# tf > 0.1 and healthy\n",
    "cv_results3 = cross_validate(model, \n",
    "                          x, y, \n",
    "                          groups= z,\n",
    "                          scoring= scoring_2,\n",
    "                          return_train_score=True, \n",
    "                          cv=kfold_grouped,\n",
    "                          verbose=2,\n",
    "                          return_estimator=True,\n",
    "                          n_jobs=1)\n",
    "\n",
    "\n",
    "# tf < 0.1 and healthy\n",
    "cv_results4 = cross_validate(model, \n",
    "                          x, y, \n",
    "                          groups=z,\n",
    "                          scoring= scoring_2,\n",
    "                          return_train_score=True, \n",
    "                          cv=kfold_grouped,\n",
    "                          verbose=2,\n",
    "                          return_estimator=True,\n",
    "                          n_jobs=1)\n",
    "\n",
    "\n",
    "\n",
    "# x_clean_n_isize_n_motif_s1_C_n_motif_s1_T_TF_0.03_0.05_.npy\n",
    "cv_results = cross_validate(model, \n",
    "                          x, y, \n",
    "                          groups=z,\n",
    "                          scoring= scoring_2,\n",
    "                          return_train_score=True, \n",
    "                          cv=kfold_grouped,\n",
    "                          verbose=2,\n",
    "                          return_estimator=True,\n",
    "                          n_jobs=1)\n",
    "\n",
    "\n",
    "# Perform repeated cross-validation with shuffling\n",
    "results = {}\n",
    "\n",
    "for _ in range(num_repeats):\n",
    "    # report which loop we are on\n",
    "    \n",
    "    # Perform cross-validation with shuffling enabled\n",
    "    cv_results = cross_validate(model, x, y, groups=z, scoring=scoring_2, cv=kfold_grouped, n_jobs=1, verbose=0)\n",
    "    \n",
    "    # Store the performance metrics for each repetition, store both train and test results\n",
    "    for key, value in cv_results.items():\n",
    "        if key not in results:\n",
    "            results[key] = []\n",
    "        results[key].append(value)\n",
    "\n",
    "# Create a pandas DataFrame from the results dictionary\n",
    "import pandas as pd\n",
    "\n",
    "# remove 'estimator' key\n",
    "relevant_keys = [k for k in cv_results.keys() if k != 'estimator']\n",
    "filtered_dict = {k: v for k, v in cv_results.items() if k in relevant_keys}\n",
    "\n",
    "# create a pandas DataFrame from the new dictionary\n",
    "cv_df = pd.DataFrame.from_dict(filtered_dict)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "cv_df.to_csv(performance_csv, index=False)\n",
    "\n",
    "# clean up memory\n",
    "del cv_results\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temp block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# or return the predicted labels\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_pred = cross_val_predict(model, x, y, cv=kfold)\n",
    "\n",
    "\n",
    "# or use for loop to do cross validation and get the metrics\n",
    "for train_index, test_index in kfold.split(x, y):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_proba = model.predict_proba(x_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    sensitivity = tp / (tp+fn)\n",
    "    f1 = 2 * (sensitivity * specificity) / (sensitivity + specificity)\n",
    "    recall = sensitivity\n",
    "    precision = tp / (tp+fp)\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    print('specificity: ', specificity)\n",
    "    print('sensitivity: ', sensitivity)\n",
    "    print('f1: ', f1)\n",
    "    print('recall: ', recall)\n",
    "    print('precision: ', precision)\n",
    "    print('accuracy: ', accuracy)\n",
    "    print('roc_auc: ', roc_auc_score(y_test, y_pred_proba))\n",
    "    print('pr_auc: ', average_precision_score(y_test, y_pred_proba))\n",
    "    print('sen_at_0.95spec: ', sensitivity_at_specificity(y_test, y_pred, 0.95))\n",
    "    print('sen_at_0.98spec: ', sensitivity_at_specificity(y_test, y_pred, 0.98))\n",
    "    print('--------------------------------------')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ROC curve of all folds into one plot and plot the mean ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "# see here: https://stackoverflow.com/questions/62739598/how-to-plot-the-roc-curve-for-ann-for-10-fold-cross-validation-in-keras-using-py\n",
    "model._estimator_type = \"classifier\"\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "\n",
    "for fold, (train, test) in enumerate(kfold.split(X = x, y = y)):\n",
    "    model.fit(x[train], y[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        model,\n",
    "        x[test],\n",
    "        y[test],\n",
    "        name=f\"ROC split {fold}\",\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=f\"Mean ROC curve with variability\\n(Positive label 'Cancer')\",\n",
    ")\n",
    "ax.axis(\"square\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#save the plt figure as pdf\n",
    "fig.savefig('roc_curve_n_isize.pdf')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
